{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition Ames Housing Prices: Team Integreat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and combine test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "# Now drop the 'Id' colum since we can not use it as a feature to train our model.\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "#Y_train = train['SalePrice']\n",
    "#X_train = train.drop('SalePrice', axis=1)\n",
    "#X_test = test.copy()\n",
    "target = ['SalePrice']\n",
    "\n",
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** initial EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inital EDA\n",
    "all_data.describe()\n",
    "\n",
    "#investigate relationships\n",
    "train.describe().columns\n",
    "cols = ['LotArea','OverallQual','OverallCond','TotalBsmtSF','GarageArea', 'SalePrice']\n",
    "sns.pairplot(train[cols], size=2)\n",
    "\n",
    "#SalePrice right-skewed - log or box cox transformation\n",
    "#TotalBsmtSF and GarageArea normally distributed\n",
    "\n",
    "#Correlations SalePrice: TotalBsmtSF, GaraArea, OverallQual\n",
    "#Multicollinearity: OverallQual and TotalBsmtSF, TotalBsmtSF and GarageArea\n",
    "\n",
    "cor = np.corrcoef(train[cols].values.T)\n",
    "sns.set(font_scale=1.5)\n",
    "heat = sns.heatmap(cor, cbar=True,\n",
    "                  annot=True,\n",
    "                  square=True,\n",
    "                  fmt='.2f',\n",
    "                  annot_kws={'size':15},\n",
    "                  yticklabels=cols,\n",
    "                  xticklabels=cols)\n",
    "\n",
    "#SalePrice correlate with: GarageArea, TotalSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate skewness\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "all_data.skew()index.plot(kind = \"barh\")\n",
    "plt.title(\"Skewness of the Continuous Numerical Features in the Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import impute, Encoder, Skewness, dummify\n",
    "\n",
    "all_data = dummify(all_data) #make dummy variables for missing values\n",
    "all_data = impute(all_data) # impute missing values\n",
    "all_data = Encoder(all_data) # encode categorical variables\n",
    "all_data = Skewness(all_data) # fix skewness of selected variables\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**split dataset again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "all_data_nomiss = all_data.copy()\n",
    "trainset = len(train)\n",
    "train = all_data_nomiss[:trainset]\n",
    "test = all_data_nomiss[trainset:]\n",
    "\n",
    "\n",
    "features = list(set(list(all_data_nomiss.columns))-set(target))\n",
    "Y_train = np.log1p(train['SalePrice'])\n",
    "X_train = train[list(features)]\n",
    "X_test = test[list(features)]\n",
    "\n",
    "#if validation within test set is wanted\n",
    "#train = train.copy()\n",
    "#train['is_train'] = np.random.uniform(0, 1, len(train)) <= .75\n",
    "#Train, Validate = train[train['is_train']==True], train[train['is_train']==False]\n",
    "\n",
    "#x_train = Train[list(features)].values\n",
    "#y_train = Train[\"SalePrice\"].values\n",
    "\n",
    "#x_validate = Validate[list(features)].values\n",
    "#y_validate = Validate[\"SalePrice\"].values\n",
    "\n",
    "#x_test=test[list(features)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**simple models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear = LinearRegression()\n",
    "\n",
    "linear.fit(X_train, Y_train)\n",
    "linear.score(X_train, Y_train)\n",
    "\n",
    "pred = linear.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice': pred})\n",
    "submission.to_csv('linear_no1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNet\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elastic = ElasticNet(alpha = 1, l1_ratio = 0.5)\n",
    "\n",
    "elastic.fit(X_train, Y_train)\n",
    "elastic.score(X_train, Y_train)\n",
    "\n",
    "pred = elastic.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice': pred})\n",
    "submission.to_csv('elastic_no1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.001, tol=0.01, random_state=1)\n",
    "\n",
    "Lasso_linear = make_pipeline(SelectFromModel(lasso_reg, prefit=False, threshold=None), LinearRegression())\n",
    "\n",
    "Lasso_linear.fit(X_train, Y_train)\n",
    "pred = Lasso_linear.predict(X_test)\n",
    "\n",
    "print('The R^2 is:', Lasso_linear.score(X_train, Y_train))\n",
    "print('CV score is:',np.mean(cross_val_score(estimator = Lasso_linear, X = X_train, y = Y_train, cv=10, n_jobs=-1)))\n",
    "print('The RMSE is:', rmse(Y_train, pred))\n",
    "\n",
    "sns.regplot(Y_train, pred, fit_reg=True, color='red')\n",
    "\n",
    "#change Y_train_preprocessed to Y_test_preprocessed for final prediction and reverse the log\n",
    "final_status = np.expm1(Lasso_linear.predict(X_test_preprocessed))\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice':final_status})\n",
    "submission.to_csv('lass0_linear.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Scaler_PCA_linear = make_pipeline(PCA(n_components = 59), LinearRegression())\n",
    "\n",
    "Scaler_PCA_linear.fit(X_train, Y_train)\n",
    "Scaler_PCA_linear.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree\n",
    "from  sklearn.treesklearn  import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, Y_train)\n",
    "tree.score(X_train, Y_train)\n",
    "\n",
    "pred = np.expm1(tree.predict(X_test))\n",
    "\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice':pred})\n",
    "submission.to_csv('tree.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feat_labels = X_train.columns[0:]\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 500, random_state=1)\n",
    "forest.fit(X_train, Y_train)\n",
    "\n",
    "#save values of importance and indices of the columns\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "indices\n",
    "\n",
    "# print the columns and importance\n",
    "for feature in range(X_train_std.shape[1]):\n",
    "    print(\"%2d) %-*s %feature\" % (feature + 1, 30, feat_labels[indices[feature]],\n",
    "                                 importances[indices[feature]]))\n",
    "    \n",
    "#visualize result with all columns\n",
    "from matplotlib import cm\n",
    "color = cm.inferno_r(np.linspace(.4,.8, 69))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "x = plt.barh(range(X_train_std.shape[1]), importances[indices], align='center', color=color)\n",
    "x = plt.yticks(range(X_train_std.shape[1]), feat_labels, rotation = 0, size=12)\n",
    "x = plt.ylim([-1, X_train_std.shape[1]])\n",
    "\n",
    "# set threshold as mean to pick features\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "importances2 = forest.feature_importances_\n",
    "indices2 = np.argsort(importances2)[::-1]\n",
    "\n",
    "sfm = SelectFromModel(forest, prefit=True, threshold=None)\n",
    "X_selected = sfm.transform(X_train_std)\n",
    "\n",
    "\n",
    "for feature in range(X_selected.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (feature + 1, 30, feat_labels[indices2[feature]],\n",
    "                                 importances2[indices2[feature]]))\n",
    "    \n",
    "#plt.figure(figsize=(20,20))\n",
    "#plt.title('Feature Importance')\n",
    "\n",
    "#x = plt.barh(range(X_selected.shape[1]), importances[indices], align='center', color=color)\n",
    "#x = plt.yticks(range(X_selected.shape[1]), feat_labels, rotation = 0, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe_tree = make_pipeline(SelectFromModel(elastic, prefit=False, threshold=None),svm())\n",
    "pipe_tree.fit(X_train, Y_train)\n",
    "pipe_tree.score(X_train, Y_train)\n",
    "\n",
    "pred = pipe_tree.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice':pred})\n",
    "submission.to_csv('elastic_svm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GridSearch with RF and Boost models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import modelfitRF\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500, random_state=1, oob_score=True, n_jobs=-1)\n",
    "\n",
    "modelfirRF(rf, X_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning n_estimators\n",
    "param_test1 = {'n_estimators':[50,70,80,90,100,200,500,1000]}\n",
    "gsearch1 = GridSearchCV(estimator = RandomForestRegressor(oob_score=True),\n",
    "                        param_grid = param_test1,n_jobs=-1,iid=False, cv=5)\n",
    "\n",
    "gsearch1.fit(X_train,Y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestRegressor(oob_score=True, n_estimators=100)\n",
    "\n",
    "modelfitRF(rf1, X_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning max_depth\n",
    "param_test2 = {'max_depth':[3,5,7,9]}\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator = RandomForestRegressor(oob_score=True, n_estimators = 100),\n",
    "                        param_grid = param_test2, n_jobs=-1,iid=False, cv=5)\n",
    "\n",
    "gsearch2.fit(X_train,Y_train)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zooming into max_depth\n",
    "param_test3 = {'max_depth':[8,9,10,11,12,15]}\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator = RandomForestRegressor(oob_score=True, n_estimators = 100),\n",
    "                        param_grid = param_test3, n_jobs=-1,iid=False, cv=5)\n",
    "\n",
    "gsearch3.fit(X_train,Y_train)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning min_samples_leaf and min_samples_size\n",
    "param_test4 = {'min_samples_split':[2,10,30,50],\n",
    "               'min_samples_leaf':[20,50,10,200,400]}\n",
    "                                    \n",
    "gsearch4 = GridSearchCV(estimator = RandomForestRegressor(oob_score=True, n_estimators = 100, max_depth=10),\n",
    "                        param_grid = param_test4, n_jobs=-1,iid=False, cv=5)\n",
    "\n",
    "gsearch4.fit(X_train,Y_train)\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfitRF(gsearch4.best_estimator_, X_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning max_features\n",
    "param_test5 = {'max_features':[10,20,30,40,50,60,70,80]}\n",
    "                                    \n",
    "gsearch5 = GridSearchCV(estimator = RandomForestRegressor(oob_score=True, n_estimators = 100, max_depth=9,\n",
    "                                                         min_samples_leaf=10, min_samples_split=10),\n",
    "                        param_grid = param_test5, n_jobs=-1,iid=False, cv=5)\n",
    "\n",
    "gsearch5.fit(X_train,Y_train)\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** final Random Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor(oob_score=True,\n",
    "                      n_estimators = 100,\n",
    "                      max_depth=9,\n",
    "                        max_features=40,\n",
    "                     min_samples_leaf=10,\n",
    "                     min_samples_split=10)\n",
    "\n",
    "\n",
    "modelfitRF(gsearch5.best_estimator_, X_train, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 4\n",
    "\n",
    "from models import modelfitxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " n_jobs=-1,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "modelfitxgb(xgb1, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning step 1\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':[3,5,7,9],\n",
    " 'min_child_weight':[1,3,5]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb1,param_grid = param_test1,n_jobs=-1,iid=False, cv=5)\n",
    "gsearch1.fit(X_train,Y_train)\n",
    "\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning step 2: zooming in\n",
    "\n",
    "param_test2 = {\n",
    " 'max_depth':[8,9,10],\n",
    " 'min_child_weight':[1]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = xgb1,param_grid = param_test2,n_jobs=-1,iid=False, cv=5)\n",
    "gsearch2.fit(X_train,Y_train)\n",
    "\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning step 2: zooming in\n",
    "\n",
    "param_test3 = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[4,5,6]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = xgb1,param_grid = param_test3,n_jobs=-1,iid=False, cv=5)\n",
    "gsearch3.fit(X_train,Y_train)\n",
    "\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning gamma\n",
    "param_test4 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=4,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " n_jobs=-1,\n",
    " scale_pos_weight=1,\n",
    " seed=27),param_grid = param_test4,n_jobs=-1,iid=False, cv=5) \n",
    "                        \n",
    "                        \n",
    "gsearch4.fit(X_train,Y_train)\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=4,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "modelfitxgb(xgb2, X_train)\n",
    "\n",
    "#max_depth: 5\n",
    "#min_child_weight: 4\n",
    "#gamma: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning subsmaple and colsample\n",
    "# take values 0.6,0.7,0.8,0.9 for both to start with\n",
    "\n",
    "param_test5 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "\n",
    "gsearch5 = GridSearchCV(estimator = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=4,\n",
    " gamma=0,\n",
    " objective= 'reg:linear',\n",
    " n_jobs=-1,\n",
    " scale_pos_weight=1,\n",
    " seed=27),param_grid = param_test5,n_jobs=-1,iid=False, cv=5) \n",
    "                        \n",
    "                        \n",
    "gsearch5.fit(X_train,Y_train)\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning subsmaple and colsample\n",
    "# go in 0.05 increments around 0.8 or 0.6 respectively\n",
    "param_test6 = {\n",
    " 'subsample':[i/100.0 for i in range(75,85,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(55,65,5)]\n",
    "}\n",
    "\n",
    "gsearch6 = GridSearchCV(estimator = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=4,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " n_jobs=-1,\n",
    " scale_pos_weight=1,\n",
    " seed=27),param_grid = param_test6,n_jobs=-1,iid=False, cv=5) \n",
    "                        \n",
    "                        \n",
    "%timeit gsearch6.fit(X_train,Y_train)\n",
    "gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch7 = GridSearchCV(estimator = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=4,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.55,\n",
    " objective= 'reg:linear',\n",
    " n_jobs=-1,\n",
    " scale_pos_weight=1,\n",
    " seed=27),param_grid = param_test7,n_jobs=-1,iid=False, cv=5) \n",
    "\n",
    "\n",
    "%timeit gsearch7.fit(X_train,Y_train)\n",
    "gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** final XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3 = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=4,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.55,\n",
    " objective= 'reg:linear',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " reg_alpha=0.01,\n",
    " seed=27)\n",
    "\n",
    "modelfitxgb(xgb3, X_train)\n",
    "\n",
    "#max_depth: 5\n",
    "#min_child_weight: 4\n",
    "#gamma: 0\n",
    "#subsample=0.8,\n",
    "#colsample_bytree=0.55,\n",
    "#reg_alpha=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GradientBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor  #GBM algorithm\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid searchc\n",
    "from models import modelfitGB\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "\n",
    "gbm0 = GradientBoostingRegressor(random_state=10)\n",
    "modelfitGB(gbm0, X_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning n_estimators\n",
    "param_test1 = {'n_estimators':[70,80,90,100]}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1,\n",
    "                                                              min_samples_split=500,\n",
    "                                                              min_samples_leaf=50,\n",
    "                                                              max_depth=8,\n",
    "                                                              max_features='sqrt',\n",
    "                                                              subsample=0.8,\n",
    "                                                              random_state=10), \n",
    "param_grid = param_test1, scoring='neg_mean_squared_error',n_jobs=-1,iid=False, cv=5)\n",
    "gsearch1.fit(X_train,Y_train)\n",
    "\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {'max_depth':[5, 7, 9, 11, 13, 15], 'min_samples_split':[200, 400, 600, 800, 1000]}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingRegressor(n_estimators=100,\n",
    "                                                              learning_rate=0.1,\n",
    "                                                              min_samples_split=500,\n",
    "                                                              min_samples_leaf=50,\n",
    "                                                              max_depth=8,\n",
    "                                                              max_features='sqrt',\n",
    "                                                              subsample=0.8,\n",
    "                                                              random_state=10), \n",
    "param_grid = param_test2, scoring='neg_mean_squared_error',n_jobs=-1,iid=False, cv=5)\n",
    "\n",
    "gsearch2.fit(X_train, Y_train)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {'min_samples_split':[50,150,200,250,300], 'min_samples_leaf':[2,5,10,20,30,40,50,60,70]}\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingRegressor(n_estimators=100,\n",
    "                                                              learning_rate=0.1,\n",
    "                                                              min_samples_split=200,\n",
    "                                                              min_samples_leaf=50,\n",
    "                                                              max_depth=7,\n",
    "                                                              max_features='sqrt',\n",
    "                                                              subsample=0.8,\n",
    "                                                              random_state=10),\n",
    "param_grid = param_test3, scoring='neg_mean_squared_error',n_jobs=-1,iid=False, cv=5)\n",
    "gsearch3.fit(X_train,Y_train)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfitGB(gsearch3.best_estimator_, X_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {'max_features':[5, 10,20,30,40,50,60,70,80]}\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = GradientBoostingRegressor(n_estimators=100,\n",
    "                                                              learning_rate=0.1,\n",
    "                                                              min_samples_split=150,\n",
    "                                                              min_samples_leaf=2,\n",
    "                                                              max_depth=7,\n",
    "                                                              max_features='sqrt',\n",
    "                                                              subsample=0.8,\n",
    "                                                              random_state=10),\n",
    "param_grid = param_test4, scoring='neg_mean_squared_error',n_jobs=-1,iid=False, cv=5)\n",
    "gsearch4.fit(X_train,Y_train)\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_\n",
    "\n",
    "#n_estimators=100\n",
    "#min_samples_split: 150\n",
    "#min_samples_leaf: 2\n",
    "#max_depth: 7\n",
    "#max_features: 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4_1 = {'max_features':[5, 6,7,8,9,10,11,12,13]}\n",
    "\n",
    "gsearch4_1 = GridSearchCV(estimator = GradientBoostingRegressor(n_estimators=100,\n",
    "                                                              learning_rate=0.1,\n",
    "                                                              min_samples_split=150,\n",
    "                                                              min_samples_leaf=2,\n",
    "                                                              max_depth=7,\n",
    "                                                              max_features='sqrt',\n",
    "                                                              subsample=0.8,\n",
    "                                                              random_state=10),\n",
    "param_grid = param_test4_1, scoring='neg_mean_squared_error',n_jobs=-1,iid=False, cv=5)\n",
    "gsearch4_1.fit(X_train,Y_train)\n",
    "gsearch4_1.grid_scores_, gsearch4_1.best_params_, gsearch4_1.best_score_\n",
    "\n",
    "#n_estimators=100\n",
    "#min_samples_split: 150\n",
    "#min_samples_leaf: 2\n",
    "#max_depth: 7\n",
    "#max_features: 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n",
    "\n",
    "gsearch5 = GridSearchCV(estimator = GradientBoostingRegressor(n_estimators=100,\n",
    "                                                              learning_rate=0.1,\n",
    "                                                              min_samples_split=150,\n",
    "                                                              min_samples_leaf=2,\n",
    "                                                              max_depth=7,\n",
    "                                                              max_features=9,\n",
    "                                                              subsample=0.8,\n",
    "                                                              random_state=10),\n",
    "param_grid = param_test5, scoring='neg_mean_squared_error',n_jobs=-1,iid=False, cv=5)\n",
    "\n",
    "gsearch5.fit(X_train,Y_train)\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** final GradientBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_tuned_1 = GradientBoostingRegressor(n_estimators=200,\n",
    "                                                              learning_rate=0.05,\n",
    "                                                              min_samples_split=150,\n",
    "                                                              min_samples_leaf=2,\n",
    "                                                              max_depth=7,\n",
    "                                                              max_features=9,\n",
    "                                                              subsample=0.8,\n",
    "                                                              random_state=10)\n",
    "modelfitGB(gbm_tuned_1, X_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wasnt able to intall Light GBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "train_data=lgb.Dataset(x_train,label=y_train)\n",
    "params = {'learning_rate':0.001}\n",
    "model= lgb.train(params, train_data, 100)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse=mean_squared_error(y_pred,y_test)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from models import modelfitKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "\n",
    "modelfitKNN(knn, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = { 'n_neighbors': [3, 5, 7, 9,11,13,17],\n",
    "                'weights' : ['uniform','distance']}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = KNeighborsRegressor(),\n",
    "                        param_grid = param_test1,n_jobs=-1,iid=False, cv=5)\n",
    "\n",
    "\n",
    "gsearch1.fit(X_train,Y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "def stacking_regression(models, meta_model, X_train, y_train, X_test,\n",
    "             transform_target=None, transform_pred=None,\n",
    "             metric=None, n_folds=3, average_fold=True,\n",
    "             shuffle=False, random_state=0, verbose=1):\n",
    "   \n",
    "    if verbose > 0:\n",
    "        print('metric: [%s]\\n' % metric.__name__)\n",
    "\n",
    "    # Split indices to get folds\n",
    "    kf = KFold(n_splits = n_folds, shuffle = shuffle, random_state = random_state)\n",
    "\n",
    "    if X_train.__class__.__name__ == \"DataFrame\":\n",
    "    \tX_train = X_train.as_matrix()\n",
    "    \tX_test = X_test.as_matrix()\n",
    "\n",
    "    # Create empty numpy arrays for stacking features\n",
    "    S_train = np.zeros((X_train.shape[0], len(models)))\n",
    "    S_test = np.zeros((X_test.shape[0], len(models)))\n",
    "\n",
    "    # Loop across models\n",
    "    for model_counter, model in enumerate(models):\n",
    "        if verbose > 0:\n",
    "            print('model %d: [%s]' % (model_counter, model.__class__.__name__))\n",
    "\n",
    "        # Create empty numpy array, which will contain temporary predictions for test set made in each fold\n",
    "        S_test_temp = np.zeros((X_test.shape[0], n_folds))\n",
    "        # Loop across folds\n",
    "        for fold_counter, (tr_index, te_index) in enumerate(kf.split(X_train, y_train)):\n",
    "            X_tr = X_train[tr_index]\n",
    "            y_tr = y_train[tr_index]\n",
    "            X_te = X_train[te_index]\n",
    "            y_te = y_train[te_index]\n",
    "            \n",
    "            # Clone the model because fit will mutate the model.\n",
    "            instance = clone(model)\n",
    "            \n",
    "            # Fit 1-st level model\n",
    "            instance.fit(X_tr, transformer(y_tr, func = transform_target))\n",
    "            \n",
    "            # Predict out-of-fold part of train set\n",
    "            S_train[te_index, model_counter] = transformer(instance.predict(X_te), func = transform_pred)\n",
    "            \n",
    "            # Predict full test set\n",
    "            S_test_temp[:, fold_counter] = transformer(instance.predict(X_test), func = transform_pred)\n",
    "\n",
    "            # Delete temperatory model\n",
    "            del instance\n",
    "\n",
    "            if verbose > 1:\n",
    "                print('    fold %d: [%.8f]' % (fold_counter, metric(y_te, S_train[te_index, model_counter])))\n",
    "\n",
    "        # Compute mean or mode of predictions for test set\n",
    "        if average_fold:\n",
    "            S_test[:, model_counter] = np.mean(S_test_temp, axis = 1)\n",
    "        else:\n",
    "            model.fit(X_train, transformer(y_train, func = transform_target))\n",
    "            S_test[:, model_counter] = transformer(model.predict(X_test), func = transform_pred)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print('    ----')\n",
    "            print('    MEAN:   [%.8f]\\n' % (metric(y_train, S_train[:, model_counter])))\n",
    "\n",
    "    # Fit our second layer meta model\n",
    "    \n",
    "    meta_model.fit(S_train, transformer(y_train, func = transform_target))\n",
    "    \n",
    "    # Make our final prediction\n",
    "    stacking_prediction = transformer(meta_model.predict(S_test), func = transform_pred)\n",
    "\n",
    "    return stacking_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # KNN\n",
    "    \n",
    "    # RandomForest\n",
    "    #RandomForestRegressor(oob_score=True,\n",
    "    #                      n_estimators = 500,\n",
    "    #                      max_depth=12,\n",
    "    #                      max_features=40,\n",
    "    #                      min_sample_split=3),\n",
    "    \n",
    "    # XGBoost\n",
    "    XGBRegressor(learning_rate =0.1,\n",
    "                 n_estimators=1000,\n",
    "                 max_depth=5,\n",
    "                 min_child_weight=4,\n",
    "                 gamma=0,\n",
    "                 subsample=0.8,\n",
    "                 colsample_bytree=0.55,\n",
    "                 objective= 'reg:linear',\n",
    "                 nthread=4,\n",
    "                 scale_pos_weight=1,\n",
    "                 reg_alpha=0.01,\n",
    "                 seed=27),\n",
    "    \n",
    "    # GradientBoost\n",
    "    GradientBoostingRegressor(n_estimators=200,\n",
    "                             learning_rate=0.05,\n",
    "                             min_samples_split=150,\n",
    "                             min_samples_leaf=2,\n",
    "                             max_depth=7,\n",
    "                              max_features=9,\n",
    "                             subsample=0.8,\n",
    "                             random_state=10),\n",
    "    \n",
    "    # Light GBoost\n",
    "    #gbr(random_state = 0, learning_rate = 0.01, max_features='sqrt',\n",
    "    #    min_samples_leaf=10, min_samples_split=5, \n",
    "    #    n_estimators = 1000, max_depth = 9)\n",
    "    ]\n",
    "\n",
    "meta_model = LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def rmse(Y_train, Y_pred):\n",
    "    rmse = sqrt(mean_squared_error(Y_train, Y_pred))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "def rmse_cv(X_train_preprocessed, Y_train):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring = 'neg_mean_squared_error', cv = 5))\n",
    "    return rmse\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_prediction = stacking_regression(models, meta_model, X_train, Y_train, X_test,\n",
    "                               transform_target=None, transform_pred = np.expm1, \n",
    "                               metric=rmsle, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'Id': test_ID, 'SalePrice':final_prediction})\n",
    "submission.to_csv('stack.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
